{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MULTI-LAYER PERCEPTRON NEURAL NETWORK V4</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspiration: A Neural Probabilistic Language Model, Yoshua Bengio\n",
    "\n",
    "character-level language model\n",
    "\n",
    "manual backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight \n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm1D:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        # fields\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # parameters\n",
    "        self.batch_weight = torch.ones(dim)\n",
    "        self.batch_bias = torch.zeros(dim)\n",
    "        # buffers\n",
    "        self.running_var = torch.ones(dim)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # find var/mean of current batch (or running batch for inference/evaluation)\n",
    "        if self.training:\n",
    "            batch_mean = x.mean(0, keepdim=True)\n",
    "            batch_var = x.var(0, keepdim=True, unbiased=True)\n",
    "        else:\n",
    "            batch_mean = self.running_mean\n",
    "            batch_var = self.running_var\n",
    "\n",
    "        x_hat = (x - batch_mean) / torch.sqrt(batch_var + self.eps) # normalized x\n",
    "        self.out = self.batch_weight * x_hat + self.batch_bias # apply batch weight and bias\n",
    "\n",
    "        # update running var/mean if training\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + (self.momentum) * batch_var\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + (self.momentum) * batch_mean\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.batch_weight, self.batch_bias]\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare pytorch gradients to manual\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    print((dt - t.grad).abs().shape)\n",
    "    max_diff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15} | exact: {str(ex):5} | approximate: {str(app):5} | maximum difference: {max_diff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"../data/names.txt\", \"r+\").read().splitlines()\n",
    "char_set = sorted(list(set(''.join(words) + '.'))) \n",
    "string_to_index = {char: ind for ind, char in enumerate(char_set)}\n",
    "index_to_string = {ind: char for char, ind in string_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182366, 3]) torch.Size([182366])\n",
      "torch.Size([22905, 3]) torch.Size([22905])\n",
      "torch.Size([22875, 3]) torch.Size([22875])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3\n",
    "vocab_size = 27\n",
    "\n",
    "def build_dataset(words):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for word in words:\n",
    "        context = [0] * block_size\n",
    "        for char in word + '.':\n",
    "            X.append(context)\n",
    "            Y.append(string_to_index[char])\n",
    "            context = context[1:] + [string_to_index[char]]\n",
    "    X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.shuffle(words)\n",
    "n1, n2 = int(len(words) * 0.8), int(len(words) * 0.9) \n",
    "X_train, Y_train = build_dataset(words[:n1])\n",
    "X_dev, Y_dev = build_dataset(words[n1:n2])\n",
    "X_test, Y_test = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "n_embed = 10 # the dimensionality of the charecter embedding vectors\n",
    "n_hidden = 200 # number of neurons in hidden layer\n",
    "\n",
    "C = torch.randn((vocab_size, n_embed)            )\n",
    "\n",
    "# layer 1\n",
    "W1 = torch.randn((n_embed * block_size, n_hidden)) * (5/3) / ((n_embed * block_size)** 0.5) \n",
    "b1 = torch.randn(n_hidden                        ) * 0.1\n",
    "\n",
    "# layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size)          ) * 0.1 \n",
    "b2 = torch.randn(vocab_size                      ) * 0.1\n",
    "\n",
    "# batch norm params\n",
    "batch_norm_gain = torch.ones((1, n_hidden)) * 0.1 + 1\n",
    "batch_norm_bias = torch.zeros((1, n_hidden)) * 0.1\n",
    "\n",
    "# running mean/std\n",
    "batch_norm_std_running = torch.ones((1, n_hidden))\n",
    "batch_norm_mean_running = torch.zeros((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, batch_norm_gain, batch_norm_bias]\n",
    "print(sum(parameter.nelement() for parameter in parameters))\n",
    "for parameter in parameters:\n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4842276573181152"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single iteration\n",
    "batch_size = 32\n",
    "indexes = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "X_batch, Y_batch = X_train[indexes], Y_train[indexes]\n",
    "\n",
    "# embed\n",
    "embed = C[X_batch]\n",
    "\n",
    "# linear layer 1\n",
    "embed_cat = embed.view(embed.shape[0], -1)\n",
    "h_preact = embed_cat @ W1 + b1\n",
    "\n",
    "# batch normalize\n",
    "batch_norm_mean = (1/batch_size) * h_preact.sum(0, keepdim=True) # calculate batch mean\n",
    "batch_norm_diff = h_preact - batch_norm_mean\n",
    "batch_norm_diff_sqr = batch_norm_diff**2\n",
    "batch_norm_var = (1/(batch_size-1)) * (batch_norm_diff_sqr).sum(0, keepdim=True) # calculate batch variance\n",
    "batch_norm_var_inv = (batch_norm_var + 1e-5)**-0.5\n",
    "batch_norm_raw = batch_norm_diff * batch_norm_var_inv\n",
    "h_preact_norm = batch_norm_gain * batch_norm_raw + batch_norm_bias # find the normalized output\n",
    "\n",
    "# non-linearity\n",
    "h = torch.tanh(h_preact_norm)\n",
    "\n",
    "# linear layer 2\n",
    "logits = h @ W2 + b2 \n",
    "\n",
    "# cross-entropy\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes\n",
    "counts = norm_logits.exp()\n",
    "sum_counts = counts.sum(1, keepdim=True)\n",
    "sum_counts_inv = sum_counts**-1\n",
    "probs = counts * sum_counts_inv \n",
    "log_probs = probs.log()\n",
    "loss = -log_probs[range(batch_size), Y_batch].mean()\n",
    "\n",
    "# backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [log_probs, probs, sum_counts_inv, sum_counts, counts, norm_logits, logit_maxes, logits, h, h_preact_norm, batch_norm_raw, batch_norm_var_inv, batch_norm_var, batch_norm_diff_sqr, batch_norm_diff, batch_norm_mean, h_preact, embed_cat, embed]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 27])\n",
      "logprobs        | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([32, 27])\n",
      "probs           | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([32, 1])\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([32, 1])\n",
      "counts_sum      | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([32, 27])\n",
      "counts          | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([32, 27])\n",
      "norm_logits     | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([32, 1])\n",
      "logit_maxes     | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([32, 27])\n",
      "logits          | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([32, 200])\n",
      "h               | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([200, 27])\n",
      "W2              | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([27])\n",
      "b2              | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([32, 200])\n",
      "hpreact         | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([1, 200])\n",
      "bngain          | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([1, 200])\n",
      "bnbias          | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([1, 200])\n",
      "bnraw           | exact: True  | approximate: True  | maximum difference: 0.0\n",
      "torch.Size([1, 200])\n",
      "bnvar_inv       | exact: False | approximate: False | maximum difference: 0.030914198607206345\n",
      "torch.Size([1, 200])\n",
      "bnvar           | exact: False | approximate: False | maximum difference: 0.007773951161652803\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "\n",
    "d_log_probs = torch.zeros_like(log_probs)\n",
    "d_log_probs[range(batch_size), Y_batch] = -1.0/batch_size\n",
    "cmp('logprobs', d_log_probs, log_probs)\n",
    "\n",
    "d_probs = (1 / probs) * d_log_probs\n",
    "cmp('probs', d_probs, probs)\n",
    "\n",
    "d_sum_counts_inv = (counts * d_probs).sum(1, keepdim=True)\n",
    "cmp('counts_sum_inv', d_sum_counts_inv, sum_counts_inv)\n",
    "\n",
    "\n",
    "d_counts = sum_counts_inv * d_probs\n",
    "d_sum_counts = (-sum_counts**-2)* d_sum_counts_inv\n",
    "cmp('counts_sum', d_sum_counts, sum_counts)\n",
    "\n",
    "d_counts += torch.ones_like(counts) * d_sum_counts\n",
    "cmp('counts', d_counts, counts)\n",
    "\n",
    "d_norm_logits = norm_logits.exp() * d_counts\n",
    "cmp('norm_logits', d_norm_logits, norm_logits)\n",
    "\n",
    "d_logit_maxes = (-1 * d_norm_logits).sum(1, keepdim=True)\n",
    "cmp('logit_maxes', d_logit_maxes, logit_maxes)\n",
    "\n",
    "d_logits = d_norm_logits\n",
    "d_logits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * d_logit_maxes\n",
    "cmp('logits' , d_logits, logits)\n",
    "\n",
    "\n",
    "d_h = d_logits @ W2.T\n",
    "cmp('h', d_h, h)\n",
    "\n",
    "d_W2 = h.T @ d_logits\n",
    "cmp('W2', d_W2, W2)\n",
    "\n",
    "d_b2 = d_logits.sum(0)\n",
    "cmp('b2', d_b2, b2)\n",
    "\n",
    "d_h_preact_norm = (1 - h**2) * d_h\n",
    "cmp('hpreact', d_h_preact_norm, h_preact_norm)\n",
    "\n",
    "\n",
    "d_batch_norm_gain = (batch_norm_raw * d_h_preact_norm).sum(0, keepdim=True)\n",
    "cmp('bngain', d_batch_norm_gain, batch_norm_gain)\n",
    "\n",
    "\n",
    "d_batch_norm_bias = d_h_preact_norm.sum(0, keepdim=True)\n",
    "cmp('bnbias', d_batch_norm_bias, batch_norm_bias)\n",
    "\n",
    "d_batch_norm_raw = d_batch_norm_gain * d_h_preact_norm \n",
    "cmp('bnraw', d_batch_norm_bias, batch_norm_bias)\n",
    "\n",
    "\n",
    "d_batch_norm_var_inv = (batch_norm_diff * d_batch_norm_raw).sum(0, ke\tepdim=True)\n",
    "cmp('bnvar_inv', d_batch_norm_var_inv, batch_norm_var_inv)\n",
    "\n",
    "d_batch_norm_var = (-0.5*batch_norm_gain**-1.5) *d_batch_norm_var_inv\n",
    "cmp('bnvar', d_batch_norm_var, batch_norm_var)\n",
    "\n",
    "\n",
    " \n",
    "# cmp('bndiff2', dbndiff2, bndiff2)\n",
    "# cmp('bndiff', dbndiff, bndiff)\n",
    "# cmp('bnmeani', dbnmeani, bnmeani)\n",
    "# cmp('hprebn', dhprebn, hprebn)\n",
    "# cmp('embcat', dembcat, embcat)\n",
    "# cmp('W1', dW1, W1)\n",
    "# cmp('b1', db1, b1)\n",
    "# cmp('emb', demb, emb)\n",
    "# cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
